# Deep_Learning Basic
## reference site
- papers with code: 논문
- google dataset: 훈련 데이터
- AI hub: 훈련 데이터

## 기계학습 목적
data를 통해 모델을 학습시키며, 일반화 성능을 최대로 하는 매개변수를 찾는 것

## representation learning
표현학습으로 좋은 특징 공간을 찾는 과정

특징 공간을 점진적으로 linear separable하도록 변화시키는 작업

## curse of dimensionality
차원의 저주

1. 차원이 증가하면서 데이터 처리량 증가
2. 데이터 요구량 증가

=> 모든 특징을 사용하지 않고, 중요한 특징들만 추출해서 사용(ex 사진을 예시로 들면 모든 pixel에 대한 data를 feature로 사용하는 것이 아닌 중요한 값이 들어있는 값을 추출해서 feature로 사용)

## basic concept
1. regression vs classificaiotn: 회귀는 예측값이 특정 숫자 값, 분류는 예측 값이 가지는 의미가 case를 의미
2. data set vs test set: 훈련 집합은 모델을 학습시키는 data, 테스트 집합은 모델의 일반화 성능을 측정하는 집합
3. modeling: 매개변수를 찾는 작업을 하기 전 function의 모양을 결정하는 과정(ex cos 함수를 사용, bias를 어느정도로 설정) <- 즉 매개변수의 수와 종류를 결정하는 과정
4. manifold hypothesis: 유사한 특징들이 매니폴드 상에서 존재, 매니폴드의 x축과 y축을 잘 변형해 샘플링하면 특징의 의미있는 변화 확인 가능

## objective function(cost function)
선형 회귀를 하기 위한 목적함수 -> 목적 함수의 값을 낮추는 것이 목표, 목적 함수의 값을 최소화하는 파라미터를 찾는 것이 기계학습의 목표

ex) MSE(mean squared error)

## over fitiing vs under fitting
over fitting: 모델의 용량이 너무 커서 훈련집합에 완벽하게 근사 -> 일반화 성능의 저하 발생 => 모델의 용량 감소 필요

과잉적합은 바이어스는 낮고, 분산은 높음

under fitiing: 모델의 용량이 작아 오차가 너무 크게 발생 => 모델의 용량 증가 필요

과소적합은 바이어스가 높고, 분산은 낮음

## regularization
- data argumentation: 주어진 데이터를 약간의 변형을 통해 여러개의 data를 생성함 -> 입력 data의 증가로 더 적합한 모델로 학습할 가능성 향상
- weight decay: 모델의 용량이 너무 커 과잉적합이 일어나는 경우 각 weight(가중치)의 값들이 크게 측정되면서 training data set 완전히 부합하는 모델로 학습할 가능성이 커짐 -> 가중치 감쇠를 통해 일반화 성능이 높은 모델로 학습하도록 유도하는 방법

